{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding text fragments\n",
    "This notebook guides you through the process of coding trade journals. These steps are as follows:\n",
    "\n",
    "1. Determine relevancy of trade journal for the study.\n",
    "2. Split journal into smaller chunks.\n",
    "3. Determine relevancy of each chunk.\n",
    "4. Code relevant chunks based on coding scheme.\n",
    "\n",
    "In this specific case, the LLM is asked to code trade journals with the aim of mapping the socio-technical configurations in the Dutch energy system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI # Replace with package required for your desired model\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field \n",
    "from langchain_openai import ChatOpenAI # Replace with package required for your desired model\n",
    "from google.cloud import bigquery\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser, SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Document\n",
    "import tiktoken\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "Here, gpt-4o-mini is used. However, any model that is compatible with LangChain's with_structured_output can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, disable_streaming=True) # Specify api key in .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load api key\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\" # Add Google Cloud credentials to key.json file\n",
    "\n",
    "# Make BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Retrieve data and transform to dataframe\n",
    "table_id = 'content.source' # Replace with table name (projectname.tablename)\n",
    "df = client.list_rows(table_id).to_dataframe()\n",
    "\n",
    "print(df.head())\n",
    "print(f\"\\nKolommen:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basemodels\n",
    "The following Pydantic basemodels are used to ensure that data fits a certain schema. The prompts are also written in these basemodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basemodel for determining relevancy of a text fragment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt based on your research goals.\n",
    "class ContentData(BaseModel):\n",
    "  \"\"\"\n",
    "  Je krijgt een dataset met daarin artikelen en stukken tekst uit artikelen van oude Nederlandse tijdschriften. Ik wil je dat een artikel/stuk uit een artikel bewaard als het voldoet aan het volgende:\n",
    "\n",
    "  Ik ben op zoek naar tekstfragmenten die relevante socio-technische configuraties beschrijven. Dit kan op twee manieren:\n",
    "  1. Discursieve focus: Fragmenten waarin actoren (zoals bedrijven, beleidsmakers of experts) uitspraken doen over de geschiktheid van bepaalde technologieën om problemen op te lossen, de regels rond het gebruik ervan, of waarin ze combinaties van technologische en institutionele elementen evalueren. Bijvoorbeeld: meningen over nieuwe technologieën, discussies over regelgeving, of uitspraken over de impact van technologie op de sector.\n",
    "  2. Substantiële focus: Fragmenten die concrete activiteiten of institutionele veranderingen beschrijven waarin technologieën en sociale elementen samenkomen. Dit kunnen berichten zijn over grote investeringen, promotieactiviteiten, het openen van nieuwe markten, de oprichting van onderzoekscentra, of andere gebeurtenissen waarin technologie en instituties samen een rol spelen.\n",
    "  Selecteer alleen die artikelen/stukken tekst met fragmenten waarin **expliciet** een combinatie van technologische en sociale/institutionele elementen wordt genoemd. Vermijd algemene beschrijvingen van technologieën zonder verwijzing naar sociale of institutionele aspecten.\n",
    "\n",
    "  Verder is het belangrijk dat het tekstfragment **direct** betrekking heeft op het **Nederlandse energiesysteem**. Dit kan zijn energie productie, transmissie, conversie, opslag of gebruik. Een trein gebruikt om kolen te vervoeren is hierbij wel relevant, maar een trein op zichzelf niet.\n",
    "\n",
    "  1. Technologieën: Technologie is het geheel van materiële en immateriële entiteiten, vaardigheden en activiteiten waarmee mensen mentale en fysieke inspanning toepassen om hun omgeving te veranderen, input omzetten in output en de toegang tot hulpbronnen verbeteren. Het is een essentieel onderdeel van het sociotechnische en socioculturele landschap en belichaamt kennis, rationaliteit en ideologie, terwijl het zowel bedoelde als onbedoelde gevolgen met zich meebrengt.\n",
    "  2. Markten: Een markt is een dynamisch en gedecentraliseerd systeem van vraag en aanbod, waarbij productieve factoren worden toegewezen via prijssignalen. Het wordt gedreven door competitie, de maximalisatie van consumentennut en het streven naar winst door producenten, wat leidt tot structurele overgangen in productie, investeringen en consumptie. Innovatie en ondernemerschap herschikken voortdurend marktstructuren door creatieve destructie, terwijl vrijwillige uitwisselingen, geleid door eigenbelang, bijdragen aan algehele efficiëntie en welzijn. \n",
    "  3. Instituties: Een institutie is een systeem van formele en informele regels die menselijk gedrag structureren, interacties vormgeven en regelmatige patronen van praktijk creëren. Deze regels, die voortkomen uit collectieve actie en interactie, functioneren als beperkingen en mogelijkheden voor gedrag, waardoor complexiteit en coördinatiekosten worden verminderd. Instellingen kunnen expliciet of impliciet, descriptief of prescriptief zijn, en omvatten formele (zoals wetten, regelgeving en voorschriften) en informele regels (zoals overtuigingen en waarden).\n",
    "  4. Actoren: Alle actoren die invloed hebben op het energiesysteem, zoals bedrijven, overheden, netbeheerders, en andere stakeholders die betrokken zijn bij de energietransitie. Wanneer je dit niet kan vinden, hoef je hier niet voor te coderen.\n",
    "\n",
    "  **Wel** relevant:\n",
    "  - Tekstfragmenten waarin actoren iets zeggen over technologieën, instituties en/of markten gerelateerd aan het energiesysteem.\n",
    "  - Tekstfragmenten over innovaties gerelateerd aan het energiesysteem.\n",
    "  - Tekstfragmenten over aanbevelingen gerelateerd aan het energiesysteem.\n",
    "  - Een fragment is alleen relevant als je expliciet kunt aanwijzen welke Nederlandse actor, technologie, markt of institutie wordt besproken. Als deze niet te identificeren zijn, is het fragment niet relevant.\n",
    "  \n",
    "  **Niet** relevant:\n",
    "  - Tekstfragmenten gerelateerd aan abonnement of tijdschriftinformatie.\n",
    "  - Tekstfragmenten over het *buitenland*. Alles wat over het buitenland (bijv. Engeland of Duitsland) gaat is niet relevant. Classificeer een tekstfragment waarin bijvoorbeeld London of een andere buitenlandse stad wordt genoemd als **niet relevant**.\n",
    "  - Tekstfragmenten die reclame bevatten.\n",
    "\n",
    "  \"\"\"\n",
    "  titel: str = Field(..., description = \"De volledig titel van het tijdschrift.\")\n",
    "  relevant: bool = Field(..., description = \"Geeft aan of een tekstfragment relevant is.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basemodel for coding relevant text fragments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt based on your research goals\n",
    "class textCoder(BaseModel):\n",
    "  \"\"\" \n",
    "  Voor een onderzoek naar het Nederlandse energiesysteem moet je stukken tekst uit oude vaktijdschriften coderen.\n",
    "\n",
    "  Stap 1: Bekijk het stuk tekst.\n",
    "\n",
    "  Stap 2: Codeer het stuk tekst met toepasselijke codes.\n",
    "  Let op:\n",
    "  - Gebruik het gegeven codeerschema om te coderen.\n",
    "  - Je mag het codeerschema veranderen op level 4 maar geen codes toevoegen op level 5.\n",
    "  - Doe hier geen aannames. Pas alleen codes toe als er expliciet iets in de tekst staat wat het toepassen van een code rechtvaardigd. \n",
    "  - Wanneer een formele institutie bediscussieerd wordt codeer je voor een formele institutie en een informele institutie.\n",
    "  - Het gebruik van water of de benoeming van waterleidingen betekent niet gelijk dat er gebruik wordt gemaakt van water als energiebron. Codeer alleen voor waterkracht wanneer dit explicitiet genoemd wordt.\n",
    "  - Bij ergens in geloven (of juist niet), argumenten voor of tegen iets, discussies en aanbevelingen codeer je ook voor informele institutie omdat dit aangeeft wat mensen hun overtuigingen zijn. Ook het toetsen van bijvoorbeeld een wet door proeven te doen kan je zien als een overtuiging.\n",
    "  - Benoemingen van mensen waarbij het onduidelijk is wie zij zijn codeer je als de vereniging waar zij lid van zijn (bijv. in het tijdschrift Het Gas van de Vereeniging van Gasfabrikanten in Nederland) of als diverse.\n",
    "  - Gebruik de code 'Gemeente onbekend' alleen als de specifieke gemeente niet duidelijk wordt uit de tekst. Anders kan je coderen met een gemeente uit het codeerschema of daar zelf een aan toevoegen.\n",
    "  \n",
    "  Stap 3: Ga na of je alle codes die toepasselijk zijn hebt toegevoegd.\n",
    "\n",
    "  Stap 4: Mocht je codes vergeten zijn, voeg deze dan nog toe.\n",
    "  Stel hierbij de volgende controle vragen:\n",
    "  - Wanneer er een of meerdere actoren benoemd worden, is hiervoor gecodeerd?\n",
    "  - Wanneer er een of meerdere technologieën genoemd worden, is hiervoor gecodeerd?\n",
    "  - Waneer er een of meerdere markten genoemd worden, is hiervoor gecodeerd?\n",
    "  - Wanneer er een of meerdere instituties genoemd worden, is hiervoor gecodeerd?\n",
    "  - Wanneer er een locatie genoemd wordt, is hiervoor gecodeerd?\n",
    "  - Wanneer er een jaartal genoemd wordt, is hiervoor gecodeerd?\n",
    "\n",
    "  Stap 5: Als er geen jaartal uit de tekst te halen is, codeer dit dan vanuit de titel van het tijdschrift.\n",
    "\n",
    "  Stap 6: Geef een uitleg over de gekozen codes.\n",
    "  \n",
    "  Let op:\n",
    "  - Benoem in je uitleg alle gegeven codes.\n",
    "\n",
    "  Voorbeeld:\n",
    "    Stuk tekst: \"De eerste kamer heeft recent en wet geïntroduceerd die bedrijven aanmoedigt om te investeren in zonne-energie. Dit heeft geleid tot een verhoogde vraag naar zonnepanelen.\"\n",
    "    Toegewezen codes:\n",
    "    - Actor > Overheid > Parlement > Eerste kamer\n",
    "    - Concept > Institutie > Formeel > Economische institutie\n",
    "    - Concept > Technologie > Zon > Productie\n",
    "    - Concept > Markt > Toepassing > Industrie\n",
    "  \"\"\"\n",
    "  titel: str = Field(..., description = \"De volledig titel van het tijdschrift.\")\n",
    "  text: str = Field(..., description=\"Een relevant stuk tekst.\")\n",
    "  codes: list[str] = Field(..., description=\"Een lijst met een of meerdere codes per tekstfragment.\")\n",
    "  uitleg: str = Field(..., description=\"Uitleg over waarom de codes passen bij het tekstfragment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide coding scheme\n",
    "A preliminary coding scheme is given to guide the model when coding the text fragments. In this notebook, the model has the freedom to make minor changes to the coding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the coding scheme\n",
    "# In this case,  codeerschema.csv is an example of what it could look like\n",
    "\n",
    "codeerschema = pd.read_csv(\"codeerschema.csv\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\", sep=\";\")\n",
    "print(codeerschema)\n",
    "code_prompt = \"Hier is het codeerschema dat je moet gebruiken om de tekst te analyseren:\\n\\n\"\n",
    "for _, row in codeerschema.iterrows():\n",
    "    code_prompt += f\"- **{row['Code level 1']} > {row['Code level 2']} > {row['Code level 3']} > {row['Code level 4']} **:\" \\\n",
    "                   f\"(Bijvoorbeeld: {row['Voorbeeld 1']}, {row['Voorbeeld 2']}, {row['Voorbeeld 3']}, {row['Voorbeeld 4']}, {row['Voorbeeld 5']})\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Below all functions are given to execute the three steps of the coding process. The functions use models that are wrapped with the different basemodels defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for chunking journal content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function counts the amount of tokens in a chunk\n",
    "# This is used in the split_content() to determine whether further chunking is necessary\n",
    "def count_tokens(text):\n",
    "    \"\"\"Counts the number of tokens in a text using tiktoken.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits the content of journals into smaller nodes based on semantic and sentence-level splitting.\n",
    "# It first performs semantic splitting to break the text into meaningful chunks with the aim of identifying articles.\n",
    "# Then chunk size is checked.\n",
    "# If a chunk is too large, it further splits it into smaller pieces using a sentence splitter.\n",
    "# The function returns a list of nodes, each representing a smaller chunk of the original content.\n",
    "\n",
    "def split_content(row):\n",
    "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "    node_parser = SemanticSplitterNodeParser(\n",
    "        buffer_size=1, breakpoint_percentile_threshold=90, embed_model=embed_model\n",
    "    )\n",
    "    sentence_splitter = SentenceSplitter(chunk_size= 400, chunk_overlap= 100) \n",
    "\n",
    "    nodes = []\n",
    "    fragments = [] \n",
    "\n",
    "    text = str(row['content'])\n",
    "    titel = str(row['full_title'])\n",
    "\n",
    "    # Step 1: Semantic splitting\n",
    "    doc = Document(text=text, metadata={\"title\": titel})\n",
    "    split_nodes = node_parser.get_nodes_from_documents([doc])\n",
    "\n",
    "    for node in split_nodes:\n",
    "        node.metadata[\"title\"] = titel\n",
    "\n",
    "        # Step 2: Check if the node > 500 tokens\n",
    "        num_tokens = count_tokens(node.text)\n",
    "        if num_tokens > 500:\n",
    "            smaller_nodes = sentence_splitter.get_nodes_from_documents([Document(text=node.text)])\n",
    "\n",
    "            for small_node in smaller_nodes:\n",
    "                small_node.metadata[\"title\"] = titel\n",
    "                nodes.append(small_node)\n",
    "                fragments.append({'title': titel, 'text': small_node.text}) \n",
    "        else:\n",
    "            nodes.append(node)\n",
    "            fragments.append({'title': titel, 'text': node.text}) \n",
    "\n",
    "    ts_fragments = pd.DataFrame(fragments)\n",
    "\n",
    "    return nodes, ts_fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for determining relevancy of text fragments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function filters relevant text fragments based on their content for the energy system.\n",
    "# It iterates through each row in the DataFrame, constructs a prompt with the fragment's title and text, and invokes a language model to determine if the fragment is relevant. \n",
    "# Relevant fragments are added to a list, and errors during processing are logged.\n",
    "relevant_llm = model.with_structured_output(ContentData, method=\"json_schema\")\n",
    "\n",
    "def filter_relevant_fragments(df):\n",
    "    relevant_fragments = [] \n",
    "    errors = [] \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing fragments\"):\n",
    "        input_text = f\"\"\"\n",
    "        Bepaal of een tekstfragment relevant is voor het energiesysteem.\n",
    "        \n",
    "        Tijdschrift: \n",
    "        {row[\"title\"]}\n",
    "        \n",
    "        Tekst:\n",
    "        {row[\"text\"]}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = relevant_llm.invoke(input_text)\n",
    "\n",
    "            if result.relevant:\n",
    "                relevant_fragments.append(row)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['title']}: {e}\")\n",
    "            errors.append({\"tekstfragment\": row[\"text\"], \"error_message\": str(e)})\n",
    "\n",
    "    return pd.DataFrame(relevant_fragments) if relevant_fragments else None, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for coding the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_llm = model.with_structured_output(textCoder, method=\"json_schema\")\n",
    "\n",
    "# This function applies text coding to journal fragments based on a given coding schema.\n",
    "# It iterates through each row in the DataFrame, constructing a prompt with the journal title and text fragment.\n",
    "# The function then invokes a language model to generate structured codes and explanations.\n",
    "# The results, including the coded text, assigned codes, and explanations, are stored in a list.\n",
    "# Any errors encountered during processing are logged.\n",
    "# The function returns a DataFrame of coded results or None if no results are found, along with any errors.\n",
    "def code_text(df):\n",
    "  results = []\n",
    "  errors = []\n",
    "\n",
    "  for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing journals\"):\n",
    "    input_text = f\"\"\" \n",
    "    Codeer tekstfragmenten, rekening houdend met de context van het fragment die in de tekst te vinden is.\n",
    "\n",
    "    Tijdschrift titel:\n",
    "    {row[\"title\"]}\n",
    "    \n",
    "    Tekstfragment:\n",
    "    {row[\"text\"]}\n",
    "\n",
    "    Gebruik dit codeerschema:\n",
    "    {code_prompt}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "      result = code_llm.invoke(input_text, timeout=120)\n",
    "\n",
    "      results.append({\n",
    "          \"full_title\": row['title'], \n",
    "          \"tekstfragmenten\": result.text,\n",
    "          \"codes\": result.codes,  \n",
    "          \"uitleg\": result.uitleg\n",
    "      })\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error processing {row['text']}: {e}\")\n",
    "      errors.append({\"tekstfragment\": row[\"text\"], \"error_message\": str(e)})\n",
    "  \n",
    "  return pd.DataFrame(results) if results else None, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for coding the text in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function processes a DataFrame in batches to apply text coding efficiently.\n",
    "# It splits the DataFrame into smaller batches (default size = 10) and iterates over them.\n",
    "# For each batch, it calls the `code_text` function to process the text fragments.\n",
    "# If coding is successful, the results are stored; any errors encountered are logged.\n",
    "# Finally, it combines all processed batches into a single DataFrame and returns it along with any errors.\n",
    "def process_in_batches(df, batch_size=10): # Adjust batch_size as desired\n",
    "  results = []\n",
    "  errors = []\n",
    "\n",
    "  num_batches = int(np.ceil(len(df) / batch_size))  # Calculate number of batches for progress bar\n",
    "\n",
    "  for batch_num in tqdm(range(num_batches), desc=\"Processing batches\", unit=\"batch\"):\n",
    "      batch = df.iloc[batch_num * batch_size: (batch_num + 1) * batch_size]\n",
    "\n",
    "      # Code fragments per batch\n",
    "      try:\n",
    "          batch_results, code_errors = code_text(batch)\n",
    "          if batch_results is not None:\n",
    "              results.append(batch_results) \n",
    "        \n",
    "          errors.extend(code_errors)\n",
    "          \n",
    "      except Exception as e:\n",
    "          print(f\"Error processing batch {batch_num}: {e}\")\n",
    "          errors.append({\"batch_num\": batch_num, \"error_message\": str(e)})\n",
    "\n",
    "  # Combine results \n",
    "  return pd.concat(results, ignore_index=True) if results else None, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "There are two ways to execute the process of coding. The first approach is great for testing on a smaller sample. The second approach uses OpenAI's Batch API. This approach is recommended when working with large amounts of data. Code for both approaches is given below.\n",
    "\n",
    "#### Approach one\n",
    "The following code executes the process of coding fragments step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: split journal content into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = split_content(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with all fragments and corresponding journal titles\n",
    "fragments = pd.DataFrame({\n",
    "    'title': [node.metadata['title'] for node in fragments],\n",
    "    'text': [node.text for node in fragments] \n",
    "})\n",
    "print(fragments.head())\n",
    "fragments.to_html('ts_fragments.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: extract relevant fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "fragments = pd.read_html('fragments.html')\n",
    "fragments = fragments[0]\n",
    "\n",
    "# Extract relevant fragments\n",
    "if fragments is not None:\n",
    "    print(f\"Aantal fragmenten: {len(fragments)}\")\n",
    "\n",
    "    relevant_fragments, fragment_errors = filter_relevant_fragments(fragments)\n",
    "    relevant_fragments.to_html('relevant_fragments.html')\n",
    "\n",
    "    # If there are any errors, save to a csv\n",
    "    if fragment_errors:\n",
    "        fragment_errors.to_csv('fragment_errors.csv')\n",
    "        print(f\"Aantal errors: {len(fragment_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: code fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "fragments = pd.read_html('relevant_fragments.html')\n",
    "fragments = fragments[0]\n",
    "\n",
    "# Function to code fragments in batches and save results\n",
    "final, final_errors = process_in_batches(fragments) \n",
    "final.to_html('ts_final.html')\n",
    "\n",
    "# If there are any errors, save to a csv\n",
    "if final_errors:\n",
    "  final_errors.to_csv('final_errors.csv')\n",
    "  print(f\"Aantal errors: {len(final_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach two: OpenAI Batch API\n",
    "OpenAI's Batch API is useful when you have a lot of requests for which you do not need immediate response. You can send a jsonl file with all requests which OpenAI executes within 24 hours (often sooner) for 50% of the cost. See https://platform.openai.com/docs/guides/batch for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.lib._pydantic import to_strict_json_schema\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: split journal content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_content(row):\n",
    "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "    node_parser = SemanticSplitterNodeParser(\n",
    "        buffer_size=1, breakpoint_percentile_threshold=90, embed_model=embed_model\n",
    "    )\n",
    "    sentence_splitter = SentenceSplitter(chunk_size= 400, chunk_overlap= 100) \n",
    "\n",
    "    nodes = []\n",
    "    fragments = [] \n",
    "\n",
    "    text = str(row['content'])\n",
    "    titel = str(row['full_title'])\n",
    "\n",
    "    # Step 1: Semantic splitting\n",
    "    doc = Document(text=text, metadata={\"title\": titel})\n",
    "    split_nodes = node_parser.get_nodes_from_documents([doc])\n",
    "\n",
    "    for node in split_nodes:\n",
    "        node.metadata[\"title\"] = titel\n",
    "\n",
    "        # Step 2: Check if the node > 500 tokens\n",
    "        num_tokens = count_tokens(node.text)\n",
    "        if num_tokens > 500:\n",
    "            smaller_nodes = sentence_splitter.get_nodes_from_documents([Document(text=node.text)])\n",
    "\n",
    "            for small_node in smaller_nodes:\n",
    "                small_node.metadata[\"title\"] = titel\n",
    "                nodes.append(small_node)\n",
    "                fragments.append({'title': titel, 'text': small_node.text}) \n",
    "        else:\n",
    "            nodes.append(node)\n",
    "            fragments.append({'title': titel, 'text': node.text}) \n",
    "\n",
    "    ts_fragments = pd.DataFrame(fragments)\n",
    "\n",
    "    return nodes, ts_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes a DataFrame of journal entries,\n",
    "# splits each journal into smaller content fragments, saves each set of fragments as a separate CSV file, \n",
    "# and finally combines all fragments into a single CSV file.\n",
    "\n",
    "all_fragments = []  \n",
    "output_dir = \"docs\"\n",
    "df =df.iloc[366:]\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing journals\"):\n",
    "\n",
    "    nodes, ts_fragments = split_content(row)\n",
    "    ts_fragments.to_csv(os.path.join(output_dir, f\"journal{_}.csv\"))\n",
    "    all_fragments.append(ts_fragments)  \n",
    "\n",
    "final_fragments = pd.concat(all_fragments, ignore_index=True)\n",
    "\n",
    "final_fragments.to_csv('final_fragments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: extract relevant fragments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.1: set up requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column as custom_id to each row of the DataFrame to match the requests to and responses from OpenAI\n",
    "df = pd.read_csv('final_fragments.csv')\n",
    "df['custom_id'] = range(len(df))\n",
    "df.to_csv('final_fragments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform basemodel into strict json schema\n",
    "schema = to_strict_json_schema(ContentData)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes the first row of a DataFrame and formats it into a structured API request for OpenAI's chat completion endpoint. \n",
    "# It constructs a task with a unique ID, sends the journal's title and content as input, and requests a relevance check using a predefined JSON schema. \n",
    "# The task is then added to a list for batch processing.\n",
    "\n",
    "# Files may be 200MB max. It might therefore be necessary to split the data in several batches.\n",
    "\n",
    "df = pd.read_csv('final_fragments.csv')\n",
    "tasks = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    title = row['full_title']\n",
    "    description = row['text']\n",
    "    \n",
    "    task = {\n",
    "        \"custom_id\": f\"task-{index}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"temperature\": 0,\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"RelevanceCheck\",\n",
    "                    \"schema\": schema\n",
    "                }\n",
    "            },\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Is dit tekstfragment relevant? Dit is de titel: {title} en de inhoud: {description}\"\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all requests in a jsonl file\n",
    "# This file can then be uploaded to https://platform.openai.com/batches to start the process\n",
    "jsonl_filename = \"requests_step2.jsonl\"\n",
    "with open(jsonl_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for task in tasks:\n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"Batch bestand opgeslagen als {jsonl_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.2: process response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The response file can be downloaded from https://platform.openai.com/batches\n",
    "# This function extracts the relevant information of each row (custom_id, title, and relevancy (true/false))\n",
    "\n",
    "batch_data = []\n",
    "\n",
    "responses = Path('responses').glob('*.jsonl')\n",
    "\n",
    "for response in responses:\n",
    "# JSONL-bestand openen en verwerken\n",
    "    with open(response, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)  # Elke regel als JSON-object laden\n",
    "            \n",
    "            # Extract custom_id, titel en relevantie\n",
    "            custom_id = data[\"custom_id\"]\n",
    "            content_str = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            try:\n",
    "                content_dict = json.loads(content_str)  # JSON-string omzetten naar dictionary\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(response.name)\n",
    "                print(custom_id)\n",
    "                print(repr(content_str))\n",
    "\n",
    "            titel = content_dict[\"titel\"]\n",
    "            relevant = content_dict[\"relevant\"]\n",
    "\n",
    "            # Toevoegen aan lijst als dictionary\n",
    "            batch_data.append({\"custom_id\": custom_id, \"relevant\": relevant})\n",
    "\n",
    "# Lijst omzetten naar DataFrame\n",
    "df = pd.DataFrame(batch_data)\n",
    "\n",
    "# DataFrame opslaan als CSV (optioneel)\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responses then need to be matched to their corresponding requests\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "df['custom_id'] = df['custom_id'].astype(str)\n",
    "df['custom_id'] = df['custom_id'].astype(str)\n",
    "df_matched = df.merge(df, how=\"left\", on=\"custom_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns and save relevant rows (=chuncks) for the next step\n",
    "df_matched = df_matched.drop(columns=[\"Unnamed: 0\"])\n",
    "step2_output = df_matched\n",
    "step3_input = df_matched[df_matched['relevant']]\n",
    "step3_input.to_csv('step3_input.csv')\n",
    "\n",
    "# To check difference between step 2 output (all chunks) and step 3 input (relevant chunks)\n",
    "print(len(step2_output))\n",
    "print(len(step3_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.1: set up requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column as custom_id to each row of the DataFrame to match the requests to and responses from OpenAI\n",
    "step3_input['custom_id'] = range(len(step3_input))\n",
    "step3_input.to_csv('step3_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform basemodel into strict json schema\n",
    "schema = to_strict_json_schema(textCoder)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes the first row of a DataFrame and formats it into a structured API request for OpenAI's chat completion endpoint. \n",
    "# It constructs a task with a unique ID, sends the journal's title and content as well as the coding scheme as input, and requests coding based on the coding scheme and JSON schema.\n",
    "# The task is then added to a list for batch processing.\n",
    "\n",
    "# Files may be 200MB max. It might therefore be necessary to split the data in several batches.\n",
    "\n",
    "step3_input = pd.read_csv(\"step3_input.csv\")\n",
    "tasks = []\n",
    "\n",
    "for index, row in step3_input.iterrows():\n",
    "\n",
    "    titel = row[\"title\"]\n",
    "    fragment = row['text']\n",
    "    schema = code_prompt\n",
    "    \n",
    "    task = {\n",
    "        \"custom_id\": f\"{index}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"temperature\": 0,\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"textCoder\",\n",
    "                    \"schema\": schema\n",
    "                }\n",
    "            },\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Codeer de tekstfragmenten, rekening houdend met de context van het fragment die in de tekst te vinden is. Dit is de titel: {titel}. Dit is het tekstfragment: {fragment}. Dit is het codeerschema: {schema}.\"\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_filename = \"requests_step3.jsonl\"\n",
    "with open(jsonl_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for task in tasks:\n",
    "        f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(f\"Batch bestand opgeslagen als {jsonl_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2: process responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script reads multiple JSONL files from the 'responses' folder,\n",
    "# parses the content of each line, extracts relevant fields like 'custom_id', 'text', 'codes', and 'uitleg',\n",
    "# and appends them to a list called 'batch_data'. \n",
    "# It also handles and reports errors if parsing fails.\n",
    "\n",
    "batch_data = []\n",
    "\n",
    "responses = Path('responses').glob('*.jsonl')\n",
    "\n",
    "for response in responses:\n",
    "# JSONL-bestand openen en verwerken\n",
    "    with open(response, \"r\") as f:\n",
    "         for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except:\n",
    "                print(line)\n",
    "                raise\n",
    "            try:\n",
    "                custom_id = data[\"custom_id\"]\n",
    "                content = textCoder.model_validate_json(data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "                text = content.text\n",
    "                codes = content.codes\n",
    "                if codes:\n",
    "                    codes = list(set(codes))\n",
    "                uitleg = content.uitleg\n",
    "                batch_data.append({\n",
    "                    \"custom_id\": custom_id,\n",
    "                    \"text\": text,\n",
    "                    \"codes\": codes,\n",
    "                    \"uitleg\": uitleg\n",
    "                })\n",
    "\n",
    "            except:\n",
    "                print(response)\n",
    "                print(data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"])\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the collected batch_data list into a pandas DataFrame\n",
    "df_codes = pd.DataFrame(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the requests and response rows\n",
    "# Save data to csv\n",
    "df_codes['custom_id'] = df_codes['custom_id'].astype(str)\n",
    "step3_input['custom_id'] = step3_input['custom_id'].astype(str)\n",
    "df_matched = df_codes.merge(step3_input, how = \"inner\", on = \"custom_id\")\n",
    "df_matched.to_csv('coded_fragments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coded_fragments csv can now be used for further analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
